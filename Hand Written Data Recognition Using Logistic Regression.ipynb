{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAaElEQVR4nMXRwRHAIAgEQEq70uiM0oiExHHGuzzySPiuh4hmv5d7ZCalgipnsZZxBBwDBh6Fo88wnA00qnGvkeNNEE9BuaDxlNoCpPDgTeeS9jlaYvMZQ88UvqBquCA3k1/cd4air+sAs5l+i33yg50AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Test='mnist_png\\\\mnist_png\\\\testing'\n",
    "Train='mnist_png\\\\mnist_png\\\\training'\n",
    "Digits=['0','1','2','3','4','5','6','7','8','9']\n",
    "Test_Path=os.path.join(os.getcwd(),Test)\n",
    "Training_Path=os.path.join(os.getcwd(),Train)\n",
    "Path=os.path.join(Training_Path,'2')\n",
    "print(Path)\n",
    "for i in os.listdir(Path):\n",
    "    display(Image(filename=os.path.join(Path,i)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickle_Files=[]\n",
    "for i in Digits:\n",
    "    Pickle_Files.append(i+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is Already Pickled \n",
      "1 is Already Pickled \n",
      "2 is Already Pickled \n",
      "3 is Already Pickled \n",
      "4 is Already Pickled \n",
      "5 is Already Pickled \n",
      "6 is Already Pickled \n",
      "7 is Already Pickled \n",
      "8 is Already Pickled \n",
      "9 is Already Pickled \n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "Corrupted_Image=np.zeros((image_size, image_size),dtype=np.float32)\n",
    "def load_letter(folder, Max_images):  \n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(Max_images, image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if np.array_equal(image_data,Corrupted_Image):\n",
    "            print('Corrupted')\n",
    "            continue\n",
    "      #print('All fine')      \n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "      if num_images==Max_images:\n",
    "        return dataset\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "for i in os.listdir(Training_Path):\n",
    "     set_filename=i+'.pickle'\n",
    "     if set_filename in os.listdir(os.getcwd()):\n",
    "        print(i+' is Already Pickled ')\n",
    "     else:\n",
    "        \n",
    "         dataset1=load_letter(os.path.join(Training_Path,i), 5000)\n",
    "         print(dataset1.shape)   \n",
    "         #print(os.path.join(Test_Path,i))\n",
    "         dataset2=load_letter(os.path.join(Test_Path,i),890)\n",
    "         print(dataset2.shape)\n",
    "         dataset = np.ndarray(shape=(dataset1.shape[0]+dataset2.shape[0], image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "         for i in range(dataset1.shape[0]):\n",
    "                dataset[i,:,:]=dataset1[i,:,:]\n",
    "         for i in range(dataset2.shape[0]):\n",
    "                dataset[dataset1.shape[0]+i,:,:]=dataset1[i,:,:]\n",
    "         print(dataset.shape)       \n",
    "         try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "              pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "         except Exception as e:\n",
    "            print('Unable to save data to', set_filename, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5890, 28, 28)\n",
      "(5890, 28, 28)\n",
      "(5890, 28, 28)\n",
      "(5890, 28, 28)\n",
      "(5890, 28, 28)\n",
      "(5890, 28, 28)\n",
      "(5890, 28, 28)\n",
      "(5890, 28, 28)\n",
      "(5890, 28, 28)\n",
      "(5890, 28, 28)\n",
      "58900\n"
     ]
    }
   ],
   "source": [
    "sum1=0\n",
    "for pickle_file in Pickle_Files:\n",
    "    f=open(pickle_file,'rb')\n",
    "    data=pickle.load(f)\n",
    "    sum1=sum1+data.shape[0]\n",
    "    print(data.shape)\n",
    "print(sum1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5890, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noAg/EBThB4L6f5IORP/FYJGPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25b191de7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle_file = Pickle_Files[2]  \n",
    "with open(pickle_file, 'rb') as f:\n",
    "    letter_set = pickle.load(f)\n",
    "    print(letter_set.shape)\n",
    "    sample_idx = np.random.randint(len(letter_set))  \n",
    "    sample_image = letter_set[sample_idx, :, :]  \n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(letter_set.shape[0]):\n",
    "    if np.array_equal(letter_set[i,:,:],np.zeros((28,28),dtype=np.float32)):\n",
    "        print(i,'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.pickle\n",
      "1 : 1.pickle\n",
      "2 : 2.pickle\n",
      "3 : 3.pickle\n",
      "4 : 4.pickle\n",
      "5 : 5.pickle\n",
      "6 : 6.pickle\n",
      "7 : 7.pickle\n",
      "8 : 8.pickle\n",
      "9 : 9.pickle\n",
      "0 : 0.pickle\n",
      "1 : 1.pickle\n",
      "2 : 2.pickle\n",
      "3 : 3.pickle\n",
      "4 : 4.pickle\n",
      "5 : 5.pickle\n",
      "6 : 6.pickle\n",
      "7 : 7.pickle\n",
      "8 : 8.pickle\n",
      "9 : 9.pickle\n",
      "Training: (50000, 28, 28) (50000,)\n",
      "Validation: (8000, 28, 28) (8000,)\n",
      "Testing: (900, 28, 28) (900,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray((nb_rows,), dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):\n",
    "    print(label,\":\",pickle_file)    \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 50000\n",
    "valid_size = 8000\n",
    "test_size = 900\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  Pickle_Files, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(Pickle_Files, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(os.getcwd(), 'Data.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 184946502\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n",
      "Training start time 1509887142.2991562\n",
      "Training End Time time 1509888323.9884963\n",
      "Time taken 1181.689842224121 seconds\n",
      "Prediction start time 1509888323.9889984\n",
      "Train_Accuracy 0.92726\n",
      "Prediction time taken 0.3058147430419922 seconds\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "clf='LogisticRegressionCLassifier.pickle'\n",
    "if clf in os.listdir(os.getcwd()):\n",
    "    print('Classifier Already Present')\n",
    "    f=open('LogisticRegressionCLassifier.pickle','rb')\n",
    "    Clf=pickle.load(f)\n",
    "    print(Clf)\n",
    "else:    \n",
    "    clf = LogisticRegressionCV()\n",
    "    train_dataset=train_dataset.reshape((train_dataset.shape[0],train_dataset.shape[1]*train_dataset.shape[2]))\n",
    "    print(train_dataset.shape)\n",
    "    print(train_labels.shape)\n",
    "    A=t.time()\n",
    "    print('Training start time',t.time())\n",
    "    clf.fit(train_dataset,train_labels)\n",
    "    print('Training End Time time',t.time())\n",
    "    print('Time taken',t.time()-A,'seconds')\n",
    "    A=t.time()\n",
    "    print('Prediction start time',t.time())\n",
    "    print('Train_Accuracy',accuracy_score(train_labels,clf.predict(train_dataset)))\n",
    "    print('Prediction time taken',t.time()-A,'seconds')\n",
    "    f=open('LogisticRegressionCLassifier.pickle','wb')\n",
    "    pickle.dump(clf,f,pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dwijraj\\Hand Written MNIST\n",
      "(900,)\n",
      "(900, 28, 28)\n",
      "Test set accuracy 0.92\n",
      "Testing time 0.006015777587890625\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(os.getcwd())\n",
    "f=open('LogisticRegressionCLassifier.pickle','rb')\n",
    "Clf=pickle.load(f)\n",
    "Data=open('Data.pickle','rb')\n",
    "test_data=pickle.load(Data)['test_dataset']\n",
    "Data.close()\n",
    "Data=open('Data.pickle','rb')\n",
    "test_labels=pickle.load(Data)['test_labels']\n",
    "print(test_labels.shape)\n",
    "print(test_data.shape)\n",
    "test_data=test_data.reshape((test_data.shape[0],test_data.shape[1]*test_data.shape[2]))\n",
    "A=t.time()\n",
    "print('Test set accuracy',accuracy_score(test_labels,Clf.predict(test_data)))\n",
    "print('Testing time',t.time()-A)\n",
    "Data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
