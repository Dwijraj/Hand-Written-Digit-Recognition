{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxklEQVR4nGNgoDUoqV39//+0WKxyK/+CwS05HHJX+zb8/VuFKWfy6+8lBR4GtnN/ezAlfX9fkgRSVT/+OmIxVl4IRF78i1USDEq//z3GhUPO5/vf5/a4NDb8/TsBl9yGb3/n8+CQk3z196UyLo3H/v7txSXn9+PvXlyGCp/Ao7Ht79+1uDQy/Pj7VxKXHEhSWwQIWBlYRURUp02bNpkLRRIMVvQvg7KqEZLr/iLAz+/fV5aWWiKZW1ZdXQ3SNLu6WhOn5QMMADeOcLN/VWHoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Test='mnist_png\\\\mnist_png\\\\testing'\n",
    "Train='mnist_png\\\\mnist_png\\\\training'\n",
    "Digits=['0','1','2','3','4','5','6','7','8','9']\n",
    "Test_Path=os.path.join(os.getcwd(),Test)\n",
    "Training_Path=os.path.join(os.getcwd(),Train)\n",
    "Path=os.path.join(Test_Path,'2')\n",
    "print(Path)\n",
    "for i in os.listdir(Path):\n",
    "    display(Image(filename=os.path.join(Path,i)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickle_Files=[]\n",
    "for i in Digits:\n",
    "    Pickle_Files.append(i+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\0\n",
      "(5923, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\0\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\0\n",
      "(6903, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\1\n",
      "(6742, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\1\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\1\n",
      "(7877, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\2\n",
      "(5958, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\2\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\2\n",
      "(6990, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\3\n",
      "(6131, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\3\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\3\n",
      "(7141, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\4\n",
      "(5842, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\4\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\4\n",
      "(6824, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\5\n",
      "(5421, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\5\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\5\n",
      "(6313, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\6\n",
      "(5918, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\6\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\6\n",
      "(6876, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\7\n",
      "(6265, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\7\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\7\n",
      "(7293, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\8\n",
      "(5851, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\8\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\8\n",
      "(6825, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\training\\9\n",
      "(5949, 28, 28)\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\9\n",
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\9\n",
      "(6958, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, Max_images):  \n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "      if num_images==Max_images:\n",
    "        return dataset\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "for i in os.listdir(Training_Path):\n",
    "     set_filename=i+'.pickle'\n",
    "     if set_filename in os.listdir(os.getcwd()):\n",
    "        print(i+' is Already Pickled ')\n",
    "     else:\n",
    "        \n",
    "         dataset1=load_letter(os.path.join(Training_Path,i), 5000)\n",
    "         print(dataset1.shape)   \n",
    "         print(os.path.join(Test_Path,i))\n",
    "         dataset2=load_letter(os.path.join(Test_Path,i),890)\n",
    "         dataset = np.ndarray(shape=(dataset1.shape[0]+dataset2.shape[0], image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "         for i in range(dataset1.shape[0]):\n",
    "                dataset[i,:,:]=dataset1[i,:,:]\n",
    "         for i in range(dataset2.shape[0]):\n",
    "                dataset[dataset1.shape[0]+i,:,:]=dataset1[i,:,:]\n",
    "         print(dataset.shape)       \n",
    "         try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "              pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "         except Exception as e:\n",
    "            print('Unable to save data to', set_filename, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD1RJREFUeJzt3X+Q1PV9x/HXm/MAgzkFBXoihkjRjjENmAumrQYboyVX\nJ2jbcSSpQWuDdkJrZpy2lk6npjPOODZqTKx2QEiIUUM7SmUy1ISStGiN6GH5Kfgj9ChQhBBUQBC4\nu3f/uK/20NvPHrvf3e8e7+dj5uZ2v+/vj/csvO67u5/d78fcXQDiGVJ0AwCKQfiBoAg/EBThB4Ii\n/EBQhB8IivADQRF+ICjCDwR1Uj0PNtSG+XCNqOchgVDe0ds64odtIOtWFX4zmy7pPklNkh5y9ztT\n6w/XCF1kl1VzSAAJq3zFgNet+Gm/mTVJ+gdJn5d0vqSZZnZ+pfsDUF/VvOafKuk1d9/i7kck/UDS\njHzaAlBr1YR/nKRtfe5vz5Ydw8xmm1mHmXUc1eEqDgcgTzV/t9/d57l7m7u3NWtYrQ8HYICqCf8O\nSeP73D8rWwZgEKgm/C9ImmRmHzWzoZKulbQ0n7YA1FrFQ33u3mVmcyT9SL1DfQvdfWNunQGoqarG\n+d19maRlOfUCoI74eC8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrw\nA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQdV1im7URlNLS8nay19Pz516+tr0bM5NRzxZ\nb3n0uWQ9ue+xY9IrHHonWe7et6/iY4MzPxAW4QeCIvxAUIQfCIrwA0ERfiAowg8EVdU4v5l1Stov\nqVtSl7u35dEUjtV0+qhk/a1HTitZe+nj305uO+Sa9N///zrSk6zP+dIXk/WUPz7nP5P1NQfOTtaf\n/8ffSNZHP7a2ZK3n4MHkthHk8SGf33b3PTnsB0Ad8bQfCKra8LukH5vZajObnUdDAOqj2qf9F7v7\nDjMbI2m5mW1295V9V8j+KMyWpOH6UJWHA5CXqs787r4j+71b0hJJU/tZZ567t7l7W7OGVXM4ADmq\nOPxmNsLMPvzubUlXSNqQV2MAaquap/1jJS0xs3f386i7P5VLVwBqruLwu/sWSZ/IsZeweqZNSda3\n/O7wZH3Dx7+VZzvH6PH0k0P39PUAhi4aWbJ2wze3pY/dsjVZ1989nSxfcNlXStbO+eKa9L4DYKgP\nCIrwA0ERfiAowg8ERfiBoAg/EJS5py/NnKcWG+UX2WV1O95g8cDWZ5L1s06q3ScjL5n7Z8n66H/f\nnqx3bU0P16VseXRysr5h2vyK911O2323JOtn3vVszY5dS6t8hfb53vT4a4YzPxAU4QeCIvxAUIQf\nCIrwA0ERfiAowg8ExRTddfDmdelLTE9sTn+99Kh3J+u7ug+VrN106R8mtx255WfJeleyWp1yX6v9\ngj6VrNsnP5as/+nix0vW1txyf3LbKV1zkvXWewbn5wD64swPBEX4gaAIPxAU4QeCIvxAUIQfCIrw\nA0Exzj9AqWmy3/zcuclt//mOv0/Wj/rJyfp/d72TrH/p639esjaqzDj+YOarNybrf3P3DSVrzbc+\nlNz22utXJOvPLBqXrHf/cm+y3gg48wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUGXH+c1soaQrJe12\n9wuyZaMkLZY0QVKnpGvc/Y3atVm8g1MnlqzdfseC5LZjm6q77v6M529O1s9eeOKO5Vdj9IOlH5eb\nLrw+ue3m9geS9fl3XZKsn3vjiTHO/11J09+37DZJK9x9kqQV2X0Ag0jZ8Lv7Sknv/zM2Q9Ki7PYi\nSVfl3BeAGqv0Nf9Yd9+Z3X5d0tic+gFQJ1W/4ee9k/2VnPDPzGabWYeZdRzV4WoPByAnlYZ/l5m1\nSlL2e3epFd19nru3uXtbs2o34SSA41Np+JdKmpXdniXpyXzaAVAvZcNvZo9J+pmk88xsu5ndKOlO\nSZeb2auSPpfdBzCIlB3nd/eZJUqX5dxLQ9t9YXPJ2rSTD1a17/bN6cGSCX/Umaz3VHX0mE5bW/rf\nU5LUni7/x+XfTNb/5Jz0fAldWzrTB6gDPuEHBEX4gaAIPxAU4QeCIvxAUIQfCIpLd2demZ+eDvqV\n9m8nqtX9De25Y0y6vn9bVfvHB7UuL/mhVEnSkL9K/5u2NqUvt/6/7Wcm62Pu70zW64EzPxAU4QeC\nIvxAUIQfCIrwA0ERfiAowg8ExTj/APVU8cXZT37rlmR93E+erXjfqNCb+5LlG7amv7G+4CPL0/u/\nvMylu+9Pl+uBMz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4f+ahzy6s2b7PXrIrWe+u2ZFR0mkt\nyfKXx1Q3D81zbQ8n619Q+voR9cCZHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCKjvOb2YLJV0pabe7\nX5Atu13SVyT9Ilttrrsvq1WT9XDzquuS9Q3T5pestW/6/eS2zVu2VtQTilPttOuDwUDO/N+VNL2f\n5fe6++TsZ1AHH4iobPjdfaWkMpclATDYVPOaf46ZrTOzhWY2MreOANRFpeF/UNJESZMl7ZR0d6kV\nzWy2mXWYWcdRHa7wcADyVlH43X2Xu3e7e4+k+ZKmJtad5+5t7t7WrGGV9gkgZxWF38xa+9y9WtKG\nfNoBUC8DGep7TNKlks4ws+2S/lbSpWY2WZJL6pR0Uw17BFADZcPv7jP7WbygBr0Uquvt5mR9SOJJ\n0r53hie3HdXVVVFPqJ3uU09O1lP/3gNR7rr/0ptV7T8PfMIPCIrwA0ERfiAowg8ERfiBoAg/EBSX\n7s6Meyr9d7CnvfIpulGMnz8ypWTtLy78UXLbaqZkl6St3zgvWf+QVlW1/zxw5geCIvxAUIQfCIrw\nA0ERfiAowg8ERfiBoBjnz8Gnxv5Pst7Zkp4OunvfvjzbCePtP7goWX/2M/eUrI0ckv4adrlR/n89\nmL5sZcuqbcl6I3zJmzM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOH+mZf2eZP3THaWn8H6u7eHk\ntlcu+b1kveeOScn6ST9ZnawPVoeuKjnRkyRpW39zQ/ex+HfuT9ZPHTL0eFt6z08PnZKsP/Dl9LTs\ntmNtxceuF878QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxCUuXt6BbPxkr4naawklzTP3e8zs1GSFkua\nIKlT0jXu/kZqXy02yi+yclMXDz4jVo5O1hdPfCpZHyJL1qdvnpGsv7blV5L1lFPXpacmf+vXjybr\nD312YbJ+86rSn4/YPC29bY/S/zerccVL6c9eDLstPc7vqzfm2U5uVvkK7fO96f9QmYGc+bsk3eru\n50v6tKSvmtn5km6TtMLdJ0lakd0HMEiUDb+773T3F7Pb+yVtkjRO0gxJi7LVFkm6qlZNAsjfcb3m\nN7MJkqZIWiVprLvvzEqvq/dlAYBBYsDhN7NTJD0u6WvufsxF57z3jYN+X6CZ2Wwz6zCzjqM6XFWz\nAPIzoPCbWbN6g/+Iuz+RLd5lZq1ZvVXS7v62dfd57t7m7m3NGpZHzwByUDb8ZmaSFkja5O59L4e6\nVNKs7PYsSU/m3x6AWhnIUN/Fkp6WtF7/f0Xjuep93f9Pks6WtFW9Q317U/s6UYf6ms6dmKwPnX8g\nWX+7K/3V0x/+2hPJejWGlPn7X+1U1Sn7e44k65c8d3Oyft15zyfri79T+v9a68q3kts26lBeOccz\n1Ff2+/zu/oxUciD6xEsyEASf8AOCIvxAUIQfCIrwA0ERfiAowg8EVXacP08n6jh/tfw3P5GsvzH3\nULKemiL83jOfTm575570sXvKfN142baPJevt40uPlz/x/WnJbccv3Jysl9P9y+THTk5IeX+lF8AJ\niPADQRF+ICjCDwRF+IGgCD8QFOEHgmKc/wTQ1NJSsrbn6vQ4/Bk/fDlZP3DJrybrJ/9L+jv1TaeP\nKlmLOA5fa4zzAyiL8ANBEX4gKMIPBEX4gaAIPxAU4QeCYpwfOIEwzg+gLMIPBEX4gaAIPxAU4QeC\nIvxAUIQfCKps+M1svJn91MxeMrONZnZLtvx2M9thZmuyn/batwsgLycNYJ0uSbe6+4tm9mFJq81s\neVa7192/Ubv2ANRK2fC7+05JO7Pb+81sk6RxtW4MQG0d12t+M5sgaYqkVdmiOWa2zswWmtnIEtvM\nNrMOM+s4qsNVNQsgPwMOv5mdIulxSV9z932SHpQ0UdJk9T4zuLu/7dx9nru3uXtbs4bl0DKAPAwo\n/GbWrN7gP+LuT0iSu+9y925375E0X9LU2rUJIG8DebffJC2QtMnd7+mzvLXPaldL2pB/ewBqZSDv\n9v+WpOskrTezNdmyuZJmmtlkSS6pU9JNNekQQE0M5N3+Z6R+J2lfln87AOqFT/gBQRF+ICjCDwRF\n+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqusU3Wb2C0lb+yw6Q9KeujVw\nfBq1t0btS6K3SuXZ20fcffRAVqxr+D9wcLMOd28rrIGERu2tUfuS6K1SRfXG034gKMIPBFV0+OcV\nfPyURu2tUfuS6K1ShfRW6Gt+AMUp+swPoCCFhN/MppvZy2b2mpndVkQPpZhZp5mtz2Ye7ii4l4Vm\nttvMNvRZNsrMlpvZq9nvfqdJK6i3hpi5OTGzdKGPXaPNeF33p/1m1iTpFUmXS9ou6QVJM939pbo2\nUoKZdUpqc/fCx4TN7DOSDkj6nrtfkC27S9Jed78z+8M50t3/skF6u13SgaJnbs4mlGntO7O0pKsk\nXa8CH7tEX9eogMetiDP/VEmvufsWdz8i6QeSZhTQR8Nz95WS9r5v8QxJi7Lbi9T7n6fuSvTWENx9\np7u/mN3eL+ndmaULfewSfRWiiPCPk7Stz/3taqwpv13Sj81stZnNLrqZfozNpk2XpNcljS2ymX6U\nnbm5nt43s3TDPHaVzHidN97w+6CL3f1CSZ+X9NXs6W1D8t7XbI00XDOgmZvrpZ+Zpd9T5GNX6YzX\neSsi/Dskje9z/6xsWUNw9x3Z792SlqjxZh/e9e4kqdnv3QX3855Gmrm5v5ml1QCPXSPNeF1E+F+Q\nNMnMPmpmQyVdK2lpAX18gJmNyN6IkZmNkHSFGm/24aWSZmW3Z0l6ssBejtEoMzeXmllaBT92DTfj\ntbvX/UdSu3rf8f+5pL8uoocSfZ0jaW32s7Ho3iQ9pt6ngUfV+97IjZJOl7RC0quS/k3SqAbq7WFJ\n6yWtU2/QWgvq7WL1PqVfJ2lN9tNe9GOX6KuQx41P+AFB8YYfEBThB4Ii/EBQhB8IivADQRF+ICjC\nDwRF+IGg/g/Dxr6NJf/PAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21abfb032b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle_file = Pickle_Files[0]  \n",
    "with open(pickle_file, 'rb') as f:\n",
    "    letter_set = pickle.load(f)  \n",
    "    sample_idx = np.random.randint(len(letter_set))  \n",
    "    sample_image = letter_set[sample_idx, :, :]  \n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.pickle\n",
      "1 : 1.pickle\n",
      "2 : 2.pickle\n",
      "3 : 3.pickle\n",
      "4 : 4.pickle\n",
      "5 : 5.pickle\n",
      "6 : 6.pickle\n",
      "7 : 7.pickle\n",
      "8 : 8.pickle\n",
      "9 : 9.pickle\n",
      "0 : 0.pickle\n",
      "1 : 1.pickle\n",
      "2 : 2.pickle\n",
      "3 : 3.pickle\n",
      "4 : 4.pickle\n",
      "5 : 5.pickle\n",
      "6 : 6.pickle\n",
      "7 : 7.pickle\n",
      "8 : 8.pickle\n",
      "9 : 9.pickle\n",
      "Training: (40000, 28, 28) (40000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray((nb_rows,), dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):\n",
    "    print(label,\":\",pickle_file)    \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 40000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  Pickle_Files, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(Pickle_Files, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(os.getcwd(), 'Data.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 188400502\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 784)\n",
      "(40000,)\n",
      "Training start time 1509137513.520136\n",
      "Training End Time time 1509138822.496654\n",
      "Time taken 1308.9805290699005 seconds\n",
      "Prediction start time 1509138822.5006652\n",
      "Train_Accuracy 0.81895\n",
      "Prediction time taken 0.7388837337493896 seconds\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "clf='LogisticRegressionCLassifier.pickle'\n",
    "if clf in os.listdir(os.getcwd()):\n",
    "    print('Classifier Already Present')\n",
    "    f=open('LogisticRegressionCLassifier.pickle','rb')\n",
    "    Clf=pickle.load(f)\n",
    "    print(Clf)\n",
    "else:    \n",
    "    clf = LogisticRegressionCV()\n",
    "    train_dataset=train_dataset.reshape((train_dataset.shape[0],train_dataset.shape[1]*train_dataset.shape[2]))\n",
    "    print(train_dataset.shape)\n",
    "    print(train_labels.shape)\n",
    "    A=t.time()\n",
    "    print('Training start time',t.time())\n",
    "    clf.fit(train_dataset,train_labels)\n",
    "    print('Training End Time time',t.time())\n",
    "    print('Time taken',t.time()-A,'seconds')\n",
    "    A=t.time()\n",
    "    print('Prediction start time',t.time())\n",
    "    print('Train_Accuracy',accuracy_score(train_labels,clf.predict(train_dataset)))\n",
    "    print('Prediction time taken',t.time()-A,'seconds')\n",
    "    f=open('LogisticRegressionCLassifier.pickle','wb')\n",
    "    pickle.dump(clf,f,pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
