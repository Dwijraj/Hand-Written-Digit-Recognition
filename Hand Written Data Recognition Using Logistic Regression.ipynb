{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxklEQVR4nGNgoDUoqV39//+0WKxyK/+CwS05HHJX+zb8/VuFKWfy6+8lBR4GtnN/ezAlfX9fkgRSVT/+OmIxVl4IRF78i1USDEq//z3GhUPO5/vf5/a4NDb8/TsBl9yGb3/n8+CQk3z196UyLo3H/v7txSXn9+PvXlyGCp/Ao7Ht79+1uDQy/Pj7VxKXHEhSWwQIWBlYRURUp02bNpkLRRIMVvQvg7KqEZLr/iLAz+/fV5aWWiKZW1ZdXQ3SNLu6WhOn5QMMADeOcLN/VWHoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Test='mnist_png\\\\mnist_png\\\\testing'\n",
    "Train='mnist_png\\\\mnist_png\\\\training'\n",
    "Digits=['0','1','2','3','4','5','6','7','8','9']\n",
    "Test_Path=os.path.join(os.getcwd(),Test)\n",
    "Training_Path=os.path.join(os.getcwd(),Train)\n",
    "Path=os.path.join(Test_Path,'2')\n",
    "print(Path)\n",
    "for i in os.listdir(Path):\n",
    "    display(Image(filename=os.path.join(Path,i)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickle_Files=[]\n",
    "for i in Digits:\n",
    "    Pickle_Files.append(i+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is Already Pickled \n",
      "1 is Already Pickled \n",
      "2 is Already Pickled \n",
      "3 is Already Pickled \n",
      "4 is Already Pickled \n",
      "5 is Already Pickled \n",
      "6 is Already Pickled \n",
      "7 is Already Pickled \n",
      "8 is Already Pickled \n",
      "9 is Already Pickled \n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, Max_images):\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "      if num_images==Max_images:\n",
    "        return dataset\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "for i in os.listdir(Training_Path):\n",
    "     set_filename=i+'.pickle'\n",
    "     if set_filename in os.listdir(os.getcwd()):\n",
    "        print(i+' is Already Pickled ')\n",
    "     else:\n",
    "        \n",
    "         dataset=load_letter(os.path.join(Training_Path,i), 5000)\n",
    "         set_filename=i+'.pickle'\n",
    "         try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "              pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "         except Exception as e:\n",
    "            print('Unable to save data to', set_filename, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmjJ+qSnjl5r6fw80T/MeHrUgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23783646588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle_file = Pickle_Files[0]  \n",
    "with open(pickle_file, 'rb') as f:\n",
    "    letter_set = pickle.load(f)  \n",
    "    sample_idx = np.random.randint(len(letter_set))  \n",
    "    sample_image = letter_set[sample_idx, :, :]  \n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.pickle\n",
      "1 : 1.pickle\n",
      "2 : 2.pickle\n",
      "3 : 3.pickle\n",
      "4 : 4.pickle\n",
      "5 : 5.pickle\n",
      "6 : 6.pickle\n",
      "7 : 7.pickle\n",
      "8 : 8.pickle\n",
      "9 : 9.pickle\n",
      "0 : 0.pickle\n",
      "1 : 1.pickle\n",
      "2 : 2.pickle\n",
      "3 : 3.pickle\n",
      "4 : 4.pickle\n",
      "5 : 5.pickle\n",
      "6 : 6.pickle\n",
      "7 : 7.pickle\n",
      "8 : 8.pickle\n",
      "9 : 9.pickle\n",
      "Training: (40000, 28, 28) (40000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray((nb_rows,), dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):\n",
    "    print(label,\":\",pickle_file)    \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 40000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  Pickle_Files, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(Pickle_Files, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(os.getcwd(), 'Data.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 188400502\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Already Present\n",
      "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "clf='LogisticRegressionCLassifier.pickle'\n",
    "if clf in os.listdir(os.getcwd()):\n",
    "    print('Classifier Already Present')\n",
    "    f=open('LogisticRegressionCLassifier.pickle','rb')\n",
    "    Clf=pickle.load(f)\n",
    "    print(Clf)\n",
    "else:    \n",
    "    clf = LogisticRegressionCV()\n",
    "    train_dataset=train_dataset.reshape((train_dataset.shape[0],train_dataset.shape[1]*train_dataset.shape[2]))\n",
    "    print(train_dataset.shape)\n",
    "    print(train_labels.shape)\n",
    "    A=t.time()\n",
    "    print('Training start time',t.time())\n",
    "    clf.fit(train_dataset,train_labels)\n",
    "    print('Training End Time time',t.time())\n",
    "    print('Time taken',t.time()-A)\n",
    "    A=t.time()\n",
    "    print('Prediction start time',t.time())\n",
    "    print('Train_Accuracy',accuracy_score(train_labels,clf.predict(train_dataset)))\n",
    "    print('Prediction time taken',t.time()-A)\n",
    "    f=open('LogisticRegressionCLassifier.pickle','wb')\n",
    "    pickle.dump(clf,f,pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
