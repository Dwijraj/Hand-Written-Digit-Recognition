{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dwijraj\\Hand Written MNIST\\mnist_png\\mnist_png\\testing\\2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxklEQVR4nGNgoDUoqV39//+0WKxyK/+CwS05HHJX+zb8/VuFKWfy6+8lBR4GtnN/ezAlfX9fkgRSVT/+OmIxVl4IRF78i1USDEq//z3GhUPO5/vf5/a4NDb8/TsBl9yGb3/n8+CQk3z196UyLo3H/v7txSXn9+PvXlyGCp/Ao7Ht79+1uDQy/Pj7VxKXHEhSWwQIWBlYRURUp02bNpkLRRIMVvQvg7KqEZLr/iLAz+/fV5aWWiKZW1ZdXQ3SNLu6WhOn5QMMADeOcLN/VWHoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Test='mnist_png\\\\mnist_png\\\\testing'\n",
    "Train='mnist_png\\\\mnist_png\\\\training'\n",
    "Digits=['0','1','2','3','4','5','6','7','8','9']\n",
    "Test_Path=os.path.join(os.getcwd(),Test)\n",
    "Training_Path=os.path.join(os.getcwd(),Train)\n",
    "Path=os.path.join(Test_Path,'2')\n",
    "print(Path)\n",
    "for i in os.listdir(Path):\n",
    "    display(Image(filename=os.path.join(Path,i)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickle_Files=[]\n",
    "for i in Digits:\n",
    "    Pickle_Files.append(i+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is Already Pickled \n",
      "1 is Already Pickled \n",
      "2 is Already Pickled \n",
      "3 is Already Pickled \n",
      "4 is Already Pickled \n",
      "5 is Already Pickled \n",
      "6 is Already Pickled \n",
      "7 is Already Pickled \n",
      "8 is Already Pickled \n",
      "9 is Already Pickled \n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, Max_images):  \n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "      if num_images==Max_images:\n",
    "        return dataset\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "for i in os.listdir(Training_Path):\n",
    "     set_filename=i+'.pickle'\n",
    "     if set_filename in os.listdir(os.getcwd()):\n",
    "        print(i+' is Already Pickled ')\n",
    "     else:\n",
    "        \n",
    "         dataset1=load_letter(os.path.join(Training_Path,i), 5000)\n",
    "         print(dataset1.shape)   \n",
    "         print(os.path.join(Test_Path,i))\n",
    "         dataset2=load_letter(os.path.join(Test_Path,i),890)\n",
    "         dataset = np.ndarray(shape=(dataset1.shape[0]+dataset2.shape[0], image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "         for i in range(dataset1.shape[0]):\n",
    "                dataset[i,:,:]=dataset1[i,:,:]\n",
    "         for i in range(dataset2.shape[0]):\n",
    "                dataset[dataset1.shape[0]+i,:,:]=dataset1[i,:,:]\n",
    "         print(dataset.shape)       \n",
    "         try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "              pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "         except Exception as e:\n",
    "            print('Unable to save data to', set_filename, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6903, 28, 28)\n",
      "(7877, 28, 28)\n",
      "(6990, 28, 28)\n",
      "(7141, 28, 28)\n",
      "(6824, 28, 28)\n",
      "(6313, 28, 28)\n",
      "(6876, 28, 28)\n",
      "(7293, 28, 28)\n",
      "(6825, 28, 28)\n",
      "(6958, 28, 28)\n",
      "70000\n"
     ]
    }
   ],
   "source": [
    "sum1=0\n",
    "for pickle_file in Pickle_Files:\n",
    "    f=open(pickle_file,'rb')\n",
    "    data=pickle.load(f)\n",
    "    sum1=sum1+data.shape[0]\n",
    "    print(data.shape)\n",
    "print(sum1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b750199550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle_file = Pickle_Files[0]  \n",
    "with open(pickle_file, 'rb') as f:\n",
    "    letter_set = pickle.load(f)  \n",
    "    sample_idx = np.random.randint(len(letter_set))  \n",
    "    sample_image = letter_set[sample_idx, :, :]  \n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.pickle\n",
      "1 : 1.pickle\n",
      "2 : 2.pickle\n",
      "3 : 3.pickle\n",
      "4 : 4.pickle\n",
      "5 : 5.pickle\n",
      "6 : 6.pickle\n",
      "7 : 7.pickle\n",
      "8 : 8.pickle\n",
      "9 : 9.pickle\n",
      "0 : 0.pickle\n",
      "1 : 1.pickle\n",
      "2 : 2.pickle\n",
      "3 : 3.pickle\n",
      "4 : 4.pickle\n",
      "5 : 5.pickle\n",
      "6 : 6.pickle\n",
      "7 : 7.pickle\n",
      "8 : 8.pickle\n",
      "9 : 9.pickle\n",
      "Training: (50000, 28, 28) (50000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray((nb_rows,), dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):\n",
    "    print(label,\":\",pickle_file)    \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 50000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  Pickle_Files, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(Pickle_Files, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(os.getcwd(), 'Data.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 219800502\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n",
      "Training start time 1509179019.4849358\n",
      "Training End Time time 1509181234.7688863\n",
      "Time taken 2215.2859559059143 seconds\n",
      "Prediction start time 1509181234.7718945\n",
      "Train_Accuracy 0.81772\n",
      "Prediction time taken 0.39003753662109375 seconds\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "clf='LogisticRegressionCLassifier.pickle'\n",
    "if clf in os.listdir(os.getcwd()):\n",
    "    print('Classifier Already Present')\n",
    "    f=open('LogisticRegressionCLassifier.pickle','rb')\n",
    "    Clf=pickle.load(f)\n",
    "    print(Clf)\n",
    "else:    \n",
    "    clf = LogisticRegressionCV()\n",
    "    train_dataset=train_dataset.reshape((train_dataset.shape[0],train_dataset.shape[1]*train_dataset.shape[2]))\n",
    "    print(train_dataset.shape)\n",
    "    print(train_labels.shape)\n",
    "    A=t.time()\n",
    "    print('Training start time',t.time())\n",
    "    clf.fit(train_dataset,train_labels)\n",
    "    print('Training End Time time',t.time())\n",
    "    print('Time taken',t.time()-A,'seconds')\n",
    "    A=t.time()\n",
    "    print('Prediction start time',t.time())\n",
    "    print('Train_Accuracy',accuracy_score(train_labels,clf.predict(train_dataset)))\n",
    "    print('Prediction time taken',t.time()-A,'seconds')\n",
    "    f=open('LogisticRegressionCLassifier.pickle','wb')\n",
    "    pickle.dump(clf,f,pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('LogisticRegressionCLassifier.pickle','rb')\n",
    "    Clf=pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
